{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Method 4: Optimized SE-CBAM CNN - FER2013 Training\n",
                "\n",
                "## C√°c k·ªπ thu·∫≠t c·∫£i ti·∫øn so v·ªõi Method 2:\n",
                "1. **CBAM (Convolutional Block Attention Module)** ‚Äî K·∫øt h·ª£p Channel Attention (SE) V√Ä Spatial Attention\n",
                "2. **Enhanced Data Augmentation** ‚Äî K·ªπ thu·∫≠t augmentation m·∫°nh h∆°n (t·ª´ Method 1 + b·ªï sung)\n",
                "3. **Focal Loss** ‚Äî X·ª≠ l√Ω m·∫•t c√¢n b·∫±ng l·ªõp (ƒë·∫∑c bi·ªát Fear, Disgust)\n",
                "4. **Cosine Annealing LR** ‚Äî Learning rate gi·∫£m m∆∞·ª£t theo h√¨nh cosine\n",
                "5. **TƒÉng Epochs l√™n 80** v·ªõi EarlyStopping patience=15\n",
                "6. **Label Smoothing = 0.15** ‚Äî C·∫£i thi·ªán generalization\n",
                "7. **Test Time Augmentation (TTA)** ‚Äî TƒÉng accuracy khi ƒë√°nh gi√°"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 1: SETUP & GPU CHECK\n",
                "# ================================================\n",
                "!nvidia-smi\n",
                "\n",
                "import tensorflow as tf\n",
                "\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "print(f\"TensorFlow: {tf.__version__}\")\n",
                "print(f\"GPUs: {gpus}\")\n",
                "\n",
                "if gpus:\n",
                "    for gpu in gpus:\n",
                "        tf.config.experimental.set_memory_growth(gpu, True)\n",
                "    print(\"‚úÖ GPU ENABLED!\")\n",
                "else:\n",
                "    print(\"‚ùå NO GPU! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 2: IMPORT LIBRARIES\n",
                "# ================================================\n",
                "import os\n",
                "import shutil\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from PIL import Image\n",
                "import warnings\n",
                "import pickle\n",
                "import json\n",
                "import math\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers, models, regularizers, backend as K\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, LearningRateScheduler\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "from sklearn.utils.class_weight import compute_class_weight\n",
                "\n",
                "print(\"‚úÖ Libraries imported!\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 3: MOUNT DRIVE\n",
                "# ================================================\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "print(\"‚úÖ Drive mounted!\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 4: EXTRACT DATASET\n",
                "# ================================================\n",
                "ZIP_PATH = '/content/drive/MyDrive/CaptoneProject/camera.zip'\n",
                "LOCAL_PATH = '/content/dataset'\n",
                "\n",
                "if not os.path.exists(LOCAL_PATH):\n",
                "    if os.path.exists(ZIP_PATH):\n",
                "        print(\"üì¶ Unzipping...\")\n",
                "        !unzip -q -o \"{ZIP_PATH}\" -d /content/\n",
                "        if os.path.exists('/content/camera'):\n",
                "            !mv /content/camera \"{LOCAL_PATH}\"\n",
                "        elif os.path.exists('/content/train') and os.path.exists('/content/test'):\n",
                "            os.makedirs(LOCAL_PATH, exist_ok=True)\n",
                "            !mv /content/train \"{LOCAL_PATH}/train\"\n",
                "            !mv /content/test \"{LOCAL_PATH}/test\"\n",
                "        print(\"‚úÖ Dataset ready at /content/dataset\")\n",
                "    else:\n",
                "        print(\"‚ùå ZIP file not found in Drive!\")\n",
                "else:\n",
                "    print(\"‚úÖ Dataset already exists locally!\")\n",
                "\n",
                "TRAIN_DIR = os.path.join(LOCAL_PATH, 'train')\n",
                "TEST_DIR = os.path.join(LOCAL_PATH, 'test')\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 5: CONFIG\n",
                "# ================================================\n",
                "IMG_SIZE = 48\n",
                "BATCH_SIZE = 64\n",
                "EPOCHS = 80\n",
                "INITIAL_LR = 0.001\n",
                "NUM_CLASSES = 7\n",
                "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
                "SEED = 42\n",
                "LABEL_SMOOTHING = 0.15\n",
                "\n",
                "np.random.seed(SEED)\n",
                "tf.random.set_seed(SEED)\n",
                "\n",
                "CHECKPOINT_DIR = '/content/drive/MyDrive/CaptoneProject/checkpoints/method4_optimized'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "BEST_MODEL_PATH = f'{CHECKPOINT_DIR}/best_model.keras'\n",
                "HISTORY_PATH = f'{CHECKPOINT_DIR}/history.pkl'\n",
                "\n",
                "print(\"‚úÖ Config set!\")\n",
                "print(f\"   EPOCHS={EPOCHS}, LR={INITIAL_LR}, LABEL_SMOOTHING={LABEL_SMOOTHING}, BATCH={BATCH_SIZE}\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 6: ENHANCED DATA AUGMENTATION\n",
                "# ================================================\n",
                "train_datagen = ImageDataGenerator(\n",
                "    rescale=1./255,\n",
                "    rotation_range=25,\n",
                "    width_shift_range=0.2,\n",
                "    height_shift_range=0.2,\n",
                "    shear_range=0.2,\n",
                "    zoom_range=0.2,\n",
                "    horizontal_flip=True,\n",
                "    brightness_range=[0.7, 1.3],\n",
                "    fill_mode='nearest',\n",
                "    validation_split=0.2\n",
                ")\n",
                "test_datagen = ImageDataGenerator(rescale=1./255)\n",
                "\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "    TRAIN_DIR, target_size=(IMG_SIZE, IMG_SIZE), color_mode='grayscale',\n",
                "    batch_size=BATCH_SIZE, class_mode='categorical', subset='training', shuffle=True, seed=SEED)\n",
                "\n",
                "validation_generator = train_datagen.flow_from_directory(\n",
                "    TRAIN_DIR, target_size=(IMG_SIZE, IMG_SIZE), color_mode='grayscale',\n",
                "    batch_size=BATCH_SIZE, class_mode='categorical', subset='validation', shuffle=False, seed=SEED)\n",
                "\n",
                "test_generator = test_datagen.flow_from_directory(\n",
                "    TEST_DIR, target_size=(IMG_SIZE, IMG_SIZE), color_mode='grayscale',\n",
                "    batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n",
                "\n",
                "print(\"‚úÖ Data Generators ready!\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 7: CLASS WEIGHTS\n",
                "# ================================================\n",
                "train_labels = train_generator.classes\n",
                "class_weights_array = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
                "class_weights = dict(enumerate(class_weights_array))\n",
                "\n",
                "print(\"‚öñÔ∏è Class Weights:\")\n",
                "for i, emotion in enumerate(EMOTIONS):\n",
                "    print(f\"   {emotion}: {class_weights[i]:.4f}\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 8: FOCAL LOSS + CBAM ATTENTION + BUILD MODEL\n",
                "# (G·ªôp t·∫•t c·∫£ custom components v√†o 1 cell)\n",
                "# ================================================\n",
                "\n",
                "# -------------------- FOCAL LOSS --------------------\n",
                "class FocalLoss(keras.losses.Loss):\n",
                "    \"\"\"\n",
                "    Focal Loss: t·∫≠p trung v√†o m·∫´u kh√≥ ph√¢n lo·∫°i.\n",
                "    FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t)\n",
                "    \"\"\"\n",
                "    def __init__(self, gamma=2.0, alpha=0.25, label_smoothing=0.15, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.gamma = gamma\n",
                "        self.alpha = alpha\n",
                "        self.label_smoothing = label_smoothing\n",
                "\n",
                "    def call(self, y_true, y_pred):\n",
                "        num_classes = tf.cast(tf.shape(y_true)[-1], dtype=tf.float32)\n",
                "        y_true = y_true * (1.0 - self.label_smoothing) + (self.label_smoothing / num_classes)\n",
                "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
                "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
                "        focal_weight = self.alpha * tf.pow(1.0 - y_pred, self.gamma)\n",
                "        return tf.reduce_sum(focal_weight * cross_entropy, axis=-1)\n",
                "\n",
                "    def get_config(self):\n",
                "        config = super().get_config()\n",
                "        config.update({'gamma': self.gamma, 'alpha': self.alpha, 'label_smoothing': self.label_smoothing})\n",
                "        return config\n",
                "\n",
                "\n",
                "# -------------------- CBAM ATTENTION --------------------\n",
                "class ChannelAttention(layers.Layer):\n",
                "    \"\"\"Channel Attention (c·∫£i ti·∫øn SE-Block): d√πng c·∫£ AvgPool + MaxPool\"\"\"\n",
                "    def __init__(self, ratio=8, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.ratio = ratio\n",
                "\n",
                "    def build(self, input_shape):\n",
                "        ch = input_shape[-1]\n",
                "        self.dense1 = layers.Dense(ch // self.ratio, activation='relu', kernel_initializer='he_normal')\n",
                "        self.dense2 = layers.Dense(ch, kernel_initializer='he_normal')\n",
                "        self.gap = layers.GlobalAveragePooling2D()\n",
                "        self.gmp = layers.GlobalMaxPooling2D()\n",
                "        super().build(input_shape)\n",
                "\n",
                "    def call(self, x):\n",
                "        ch = x.shape[-1]\n",
                "        avg = self.dense2(self.dense1(self.gap(x)))\n",
                "        mx  = self.dense2(self.dense1(self.gmp(x)))\n",
                "        att = tf.sigmoid(avg + mx)\n",
                "        return x * tf.reshape(att, (-1, 1, 1, ch))\n",
                "\n",
                "    def get_config(self):\n",
                "        config = super().get_config()\n",
                "        config.update({'ratio': self.ratio})\n",
                "        return config\n",
                "\n",
                "\n",
                "class SpatialAttention(layers.Layer):\n",
                "    \"\"\"Spatial Attention: t·∫≠p trung v√†o v√πng quan tr·ªçng (m·∫Øt, mi·ªáng...)\"\"\"\n",
                "    def __init__(self, kernel_size=7, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.kernel_size = kernel_size\n",
                "\n",
                "    def build(self, input_shape):\n",
                "        self.conv = layers.Conv2D(1, self.kernel_size, padding='same',\n",
                "                                  activation='sigmoid', kernel_initializer='he_normal')\n",
                "        super().build(input_shape)\n",
                "\n",
                "    def call(self, x):\n",
                "        avg = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
                "        mx  = tf.reduce_max(x, axis=-1, keepdims=True)\n",
                "        att = self.conv(tf.concat([avg, mx], axis=-1))\n",
                "        return x * att\n",
                "\n",
                "    def get_config(self):\n",
                "        config = super().get_config()\n",
                "        config.update({'kernel_size': self.kernel_size})\n",
                "        return config\n",
                "\n",
                "\n",
                "class CBAMBlock(layers.Layer):\n",
                "    \"\"\"CBAM = Channel Attention ‚Üí Spatial Attention (tu·∫ßn t·ª±)\"\"\"\n",
                "    def __init__(self, ratio=8, kernel_size=7, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.ratio = ratio\n",
                "        self.kernel_size = kernel_size\n",
                "        self.ca = ChannelAttention(ratio=ratio)\n",
                "        self.sa = SpatialAttention(kernel_size=kernel_size)\n",
                "\n",
                "    def call(self, x):\n",
                "        return self.sa(self.ca(x))\n",
                "\n",
                "    def get_config(self):\n",
                "        config = super().get_config()\n",
                "        config.update({'ratio': self.ratio, 'kernel_size': self.kernel_size})\n",
                "        return config\n",
                "\n",
                "\n",
                "# -------------------- BUILD MODEL --------------------\n",
                "def build_optimized_cbam_cnn(input_shape=(48, 48, 1), num_classes=7):\n",
                "    inputs = layers.Input(shape=input_shape)\n",
                "\n",
                "    # Block 1: 64 filters\n",
                "    x = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(inputs)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = CBAMBlock(ratio=8)(x)\n",
                "    x = layers.MaxPooling2D(2)(x)       # 48‚Üí24\n",
                "    x = layers.Dropout(0.25)(x)\n",
                "\n",
                "    # Block 2: 128 filters\n",
                "    x = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = CBAMBlock(ratio=8)(x)\n",
                "    x = layers.MaxPooling2D(2)(x)       # 24‚Üí12\n",
                "    x = layers.Dropout(0.25)(x)\n",
                "\n",
                "    # Block 3: 256 filters\n",
                "    x = layers.Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = CBAMBlock(ratio=16)(x)\n",
                "    x = layers.MaxPooling2D(2)(x)       # 12‚Üí6\n",
                "    x = layers.Dropout(0.3)(x)\n",
                "\n",
                "    # Block 4: 512 filters\n",
                "    x = layers.Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = CBAMBlock(ratio=16)(x)\n",
                "    x = layers.MaxPooling2D(2)(x)       # 6‚Üí3\n",
                "    x = layers.Dropout(0.3)(x)\n",
                "\n",
                "    # Classifier\n",
                "    x = layers.GlobalAveragePooling2D()(x)\n",
                "    x = layers.Dense(512, kernel_regularizer=regularizers.l2(0.0005))(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Dropout(0.5)(x)\n",
                "    x = layers.Dense(256, kernel_regularizer=regularizers.l2(0.0005))(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Dropout(0.5)(x)\n",
                "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
                "\n",
                "    return models.Model(inputs=inputs, outputs=outputs)\n",
                "\n",
                "\n",
                "model = build_optimized_cbam_cnn()\n",
                "model.summary()\n",
                "print(f\"\\n‚úÖ Model built! Total params: {model.count_params():,}\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 9: COSINE ANNEALING LR + COMPILE\n",
                "# ================================================\n",
                "\n",
                "def cosine_annealing_schedule(epoch, initial_lr=INITIAL_LR, total_epochs=EPOCHS, min_lr=1e-6):\n",
                "    \"\"\"lr = min_lr + 0.5*(initial_lr - min_lr)*(1 + cos(pi*epoch/total_epochs))\"\"\"\n",
                "    return min_lr + (initial_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * epoch / total_epochs))\n",
                "\n",
                "# Visualize\n",
                "lrs = [cosine_annealing_schedule(e) for e in range(EPOCHS)]\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(lrs, 'b-', linewidth=2)\n",
                "plt.title('Cosine Annealing Learning Rate Schedule', fontsize=14)\n",
                "plt.xlabel('Epoch'); plt.ylabel('Learning Rate')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "# Compile\n",
                "model.compile(\n",
                "    optimizer=Adam(learning_rate=INITIAL_LR),\n",
                "    loss=FocalLoss(gamma=2.0, alpha=0.25, label_smoothing=LABEL_SMOOTHING),\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "print(f\"‚úÖ Compiled! Focal Loss (Œ≥=2.0, Œ±=0.25), Label Smoothing={LABEL_SMOOTHING}\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 10: CALLBACKS\n",
                "# ================================================\n",
                "\n",
                "class SaveHistoryCallback(Callback):\n",
                "    def __init__(self, path):\n",
                "        super().__init__()\n",
                "        self.path = path\n",
                "        self.data = {'accuracy':[], 'val_accuracy':[], 'loss':[], 'val_loss':[], 'lr':[]}\n",
                "\n",
                "    def on_epoch_end(self, epoch, logs=None):\n",
                "        for k in ['accuracy','val_accuracy','loss','val_loss']:\n",
                "            self.data[k].append(logs.get(k))\n",
                "        self.data['lr'].append(float(self.model.optimizer.learning_rate.numpy()))\n",
                "        with open(self.path, 'wb') as f:\n",
                "            pickle.dump(self.data, f)\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(BEST_MODEL_PATH, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1),\n",
                "    EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1),\n",
                "    LearningRateScheduler(cosine_annealing_schedule, verbose=1),\n",
                "    SaveHistoryCallback(HISTORY_PATH)\n",
                "]\n",
                "print(\"‚úÖ Callbacks: ModelCheckpoint, EarlyStopping(patience=15), CosineAnnealing, SaveHistory\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 11: TRAINING üöÄ\n",
                "# ================================================\n",
                "\n",
                "print(\"üöÄ Starting Training (Method 4 - Optimized)...\")\n",
                "print(f\"   Epochs={EPOCHS}, Batch={BATCH_SIZE}, Train={train_generator.samples}, Val={validation_generator.samples}\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    epochs=EPOCHS,\n",
                "    validation_data=validation_generator,\n",
                "    callbacks=callbacks,\n",
                "    class_weight=class_weights,\n",
                "    verbose=1\n",
                ")\n",
                "print(\"\\n‚úÖ Training Completed!\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 12: TRAINING VISUALIZATION\n",
                "# ================================================\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
                "\n",
                "axes[0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
                "axes[0].plot(history.history['val_accuracy'], label='Val', linewidth=2)\n",
                "axes[0].set_title('Accuracy', fontsize=14)\n",
                "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Accuracy')\n",
                "axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].plot(history.history['loss'], label='Train', linewidth=2)\n",
                "axes[1].plot(history.history['val_loss'], label='Val', linewidth=2)\n",
                "axes[1].set_title('Loss', fontsize=14)\n",
                "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss')\n",
                "axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "try:\n",
                "    with open(HISTORY_PATH, 'rb') as f:\n",
                "        lr_vals = pickle.load(f).get('lr', [])\n",
                "    if lr_vals:\n",
                "        axes[2].plot(lr_vals, 'g-', linewidth=2)\n",
                "        axes[2].set_title('Learning Rate (Cosine)', fontsize=14)\n",
                "        axes[2].set_xlabel('Epoch'); axes[2].set_ylabel('LR')\n",
                "        axes[2].grid(True, alpha=0.3)\n",
                "except:\n",
                "    pass\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{CHECKPOINT_DIR}/training_curves.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"üìä Best Val Accuracy: {max(history.history['val_accuracy'])*100:.2f}% (Epoch {np.argmax(history.history['val_accuracy'])+1})\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 13: STANDARD EVALUATION\n",
                "# ================================================\n",
                "\n",
                "print(\"üìä Standard Evaluation...\")\n",
                "best_model = keras.models.load_model(\n",
                "    BEST_MODEL_PATH,\n",
                "    custom_objects={'FocalLoss': FocalLoss, 'CBAMBlock': CBAMBlock,\n",
                "                    'ChannelAttention': ChannelAttention, 'SpatialAttention': SpatialAttention}\n",
                ")\n",
                "\n",
                "test_generator.reset()\n",
                "test_loss, test_acc = best_model.evaluate(test_generator)\n",
                "print(f\"\\nüéØ TEST ACCURACY (Standard): {test_acc*100:.2f}%\")\n",
                "print(f\"   TEST LOSS: {test_loss:.4f}\")\n",
                "\n",
                "test_generator.reset()\n",
                "predictions = best_model.predict(test_generator, verbose=1)\n",
                "y_pred = np.argmax(predictions, axis=1)\n",
                "y_true = test_generator.classes\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üìã Classification Report (Standard):\")\n",
                "print(\"=\"*60)\n",
                "print(classification_report(y_true, y_pred, target_names=EMOTIONS, digits=4))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 14: TEST TIME AUGMENTATION (TTA)\n",
                "# ================================================\n",
                "\n",
                "def predict_with_tta(model, test_dir, img_size=48, batch_size=64):\n",
                "    tta_datagens = [\n",
                "        ImageDataGenerator(rescale=1./255),\n",
                "        ImageDataGenerator(rescale=1./255, horizontal_flip=True),\n",
                "        ImageDataGenerator(rescale=1./255, rotation_range=10),\n",
                "        ImageDataGenerator(rescale=1./255, zoom_range=0.1),\n",
                "        ImageDataGenerator(rescale=1./255, width_shift_range=0.1, height_shift_range=0.1),\n",
                "    ]\n",
                "    all_preds = []\n",
                "    for i, dg in enumerate(tta_datagens):\n",
                "        print(f\"   TTA {i+1}/{len(tta_datagens)}...\")\n",
                "        gen = dg.flow_from_directory(test_dir, target_size=(img_size,img_size),\n",
                "                                     color_mode='grayscale', batch_size=batch_size,\n",
                "                                     class_mode='categorical', shuffle=False)\n",
                "        all_preds.append(model.predict(gen, verbose=0))\n",
                "    return np.mean(all_preds, axis=0)\n",
                "\n",
                "print(\"üîÑ Running TTA...\")\n",
                "tta_preds = predict_with_tta(best_model, TEST_DIR)\n",
                "y_pred_tta = np.argmax(tta_preds, axis=1)\n",
                "y_true_tta = test_generator.classes\n",
                "\n",
                "tta_accuracy = np.mean(y_pred_tta == y_true_tta)\n",
                "print(f\"\\nüèÜ TEST ACCURACY (TTA): {tta_accuracy*100:.2f}%\")\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üìã Classification Report (TTA):\")\n",
                "print(\"=\"*60)\n",
                "print(classification_report(y_true_tta, y_pred_tta, target_names=EMOTIONS, digits=4))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 15: CONFUSION MATRIX\n",
                "# ================================================\n",
                "\n",
                "cm = confusion_matrix(y_true_tta, y_pred_tta)\n",
                "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
                "\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=EMOTIONS, yticklabels=EMOTIONS, ax=axes[0])\n",
                "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14)\n",
                "axes[0].set_ylabel('True'); axes[0].set_xlabel('Predicted')\n",
                "\n",
                "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Oranges',\n",
                "            xticklabels=EMOTIONS, yticklabels=EMOTIONS, ax=axes[1])\n",
                "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14)\n",
                "axes[1].set_ylabel('True'); axes[1].set_xlabel('Predicted')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{CHECKPOINT_DIR}/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 16: SO S√ÅNH & SAVE MODEL\n",
                "# ================================================\n",
                "\n",
                "print(\"=\"*65)\n",
                "print(\"üìä SO S√ÅNH K·∫æT QU·∫¢ C√ÅC PH∆Ø∆†NG PH√ÅP\")\n",
                "print(\"=\"*65)\n",
                "\n",
                "results = {\n",
                "    'Method 1 - Enhanced Augmentation':        62.93,\n",
                "    'Method 2 - SE Attention CNN':             64.22,\n",
                "    'Method 3 - MobileNetV2':                  36.28,\n",
                "    'Method 4 - CBAM (Standard)':  test_acc * 100,\n",
                "    'Method 4 - CBAM (TTA)':       tta_accuracy * 100,\n",
                "}\n",
                "\n",
                "for method, acc in results.items():\n",
                "    bar = '‚ñà' * int(acc / 2)\n",
                "    print(f\"   {method:<42s} | {acc:6.2f}% | {bar}\")\n",
                "\n",
                "print(\"=\"*65)\n",
                "print(f\"\\nüèÜ Improvement over Method 2: +{(tta_accuracy*100 - 64.22):.2f}%\")\n",
                "\n",
                "# Save final model\n",
                "FINAL_MODEL_PATH = '/content/drive/MyDrive/CaptoneProject/best_model_method4.keras'\n",
                "best_model.save(FINAL_MODEL_PATH)\n",
                "print(f\"\\nüíæ Model saved to: {FINAL_MODEL_PATH}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üìù TRAINING SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "print(f\"   Architecture: CNN 4-Block + CBAM Attention\")\n",
                "print(f\"   Loss: Focal Loss (gamma=2.0, alpha=0.25)\")\n",
                "print(f\"   Label Smoothing: {LABEL_SMOOTHING}\")\n",
                "print(f\"   LR: Cosine Annealing ({INITIAL_LR} ‚Üí 1e-6)\")\n",
                "print(f\"   Augmentation: Enhanced (rot=25¬∞, shift=0.2, brightness)\")\n",
                "print(f\"   Params: {model.count_params():,}\")\n",
                "print(f\"   Best Val Acc: {max(history.history['val_accuracy'])*100:.2f}%\")\n",
                "print(f\"   Test Acc (Standard): {test_acc*100:.2f}%\")\n",
                "print(f\"   Test Acc (TTA): {tta_accuracy*100:.2f}%\")\n",
                "print(\"=\"*60)\n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        },
        "accelerator": "GPU",
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}