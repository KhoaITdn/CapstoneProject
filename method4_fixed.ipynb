{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Method 4 FIXED: Optimized CBAM CNN - FER2013\n",
                "\n",
                "## üîç Ph√¢n t√≠ch v·∫•n ƒë·ªÅ t·ª´ l·∫ßn ch·∫°y tr∆∞·ªõc:\n",
                "- **Val Accuracy ch·ªâ ƒë·∫°t 46.3%** ‚Äî Th·∫•p h∆°n Method 2 (64.22%)\n",
                "- **Nguy√™n nh√¢n 1**: Focal Loss (gamma=2.0) + Label Smoothing (0.15) qu√° aggressive ‚Üí gradient r·∫•t nh·ªè ‚Üí model h·ªçc ch·∫≠m/k√©m\n",
                "- **Nguy√™n nh√¢n 2**: Cosine Annealing LR v·ªõi min_lr=1e-6 ‚Üí LR gi·∫£m qu√° nhanh ‚Üí model b·ªã underfitting s·ªõm\n",
                "- **B·∫±ng ch·ª©ng**: Epoch 1 val_accuracy=7.4% (b√¨nh th∆∞·ªùng ph·∫£i ~14%), val_accuracy dao ƒë·ªông r·∫•t m·∫°nh\n",
                "\n",
                "## ‚úÖ Fix √°p d·ª•ng trong version n√†y:\n",
                "| Th√†nh ph·∫ßn | Tr∆∞·ªõc (l·ªói) | Sau (fix) | L√Ω do |\n",
                "|---|---|---|---|\n",
                "| **Loss** | FocalLoss(gamma=2.0, Œ±=0.25) | CrossEntropy + label_smoothing=0.1 | FL qu√° aggressive, gradient b·ªã tri·ªát ti√™u |\n",
                "| **LR Schedule** | Cosine Annealing (min=1e-6) | ReduceLROnPlateau (factor=0.5, patience=5) | Th√≠ch nghi v·ªõi th·ª±c t·∫ø training t·ªët h∆°n |\n",
                "| **Initial LR** | 0.001 | 0.001 | Gi·ªØ nguy√™n |\n",
                "| **Label Smoothing** | 0.15 | 0.1 | Gi·∫£m ƒë·ªÉ gradient ƒë·ªß l·ªõn |\n",
                "| **Architecture** | CBAM 4-Block | CBAM 4-Block | **GI·ªÆ NGUY√äN** ‚Äî kh√¥ng ph·∫£i nguy√™n nh√¢n |\n",
                "| **Dropout** | 0.25/0.3/0.5 | 0.25/0.3/0.5 | **GI·ªÆ NGUY√äN** |\n",
                "| **Class Weights** | C√≥ | C√≥ | **GI·ªÆ NGUY√äN** |\n",
                "| **EarlyStopping** | patience=15 | patience=15 | **GI·ªÆ NGUY√äN** |"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 1: SETUP & GPU CHECK\n",
                "# ================================================\n",
                "!nvidia-smi\n",
                "\n",
                "import tensorflow as tf\n",
                "\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "print(f\"TensorFlow: {tf.__version__}\")\n",
                "print(f\"GPUs: {gpus}\")\n",
                "\n",
                "if gpus:\n",
                "    for gpu in gpus:\n",
                "        tf.config.experimental.set_memory_growth(gpu, True)\n",
                "    print(\"‚úÖ GPU ENABLED!\")\n",
                "else:\n",
                "    print(\"‚ùå NO GPU! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 2: IMPORT LIBRARIES\n",
                "# ================================================\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import pickle\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers, models, regularizers, backend as K\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.callbacks import (\n",
                "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
                ")\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "from sklearn.utils.class_weight import compute_class_weight\n",
                "\n",
                "print(\"‚úÖ Libraries imported!\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 3: MOUNT DRIVE\n",
                "# ================================================\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "print(\"‚úÖ Drive mounted!\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 4: EXTRACT DATASET\n",
                "# ================================================\n",
                "ZIP_PATH = '/content/drive/MyDrive/CaptoneProject/camera.zip'\n",
                "LOCAL_PATH = '/content/dataset'\n",
                "\n",
                "if not os.path.exists(LOCAL_PATH):\n",
                "    if os.path.exists(ZIP_PATH):\n",
                "        print(\"üì¶ Unzipping...\")\n",
                "        !unzip -q -o \"{ZIP_PATH}\" -d /content/\n",
                "        if os.path.exists('/content/camera'):\n",
                "            !mv /content/camera \"{LOCAL_PATH}\"\n",
                "        elif os.path.exists('/content/train') and os.path.exists('/content/test'):\n",
                "            os.makedirs(LOCAL_PATH, exist_ok=True)\n",
                "            !mv /content/train \"{LOCAL_PATH}/train\"\n",
                "            !mv /content/test \"{LOCAL_PATH}/test\"\n",
                "        print(\"‚úÖ Dataset ready at /content/dataset\")\n",
                "    else:\n",
                "        print(\"‚ùå ZIP file not found in Drive!\")\n",
                "else:\n",
                "    print(\"‚úÖ Dataset already exists locally!\")\n",
                "\n",
                "TRAIN_DIR = os.path.join(LOCAL_PATH, 'train')\n",
                "TEST_DIR  = os.path.join(LOCAL_PATH, 'test')\n",
                "print(f\"Train: {TRAIN_DIR}\")\n",
                "print(f\"Test:  {TEST_DIR}\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 5: CONFIG\n",
                "# ================================================\n",
                "IMG_SIZE        = 48\n",
                "BATCH_SIZE      = 64\n",
                "EPOCHS          = 100      # TƒÉng l√™n 100, EarlyStopping s·∫Ω d·ª´ng ƒë√∫ng l√∫c\n",
                "INITIAL_LR      = 0.001\n",
                "NUM_CLASSES     = 7\n",
                "EMOTIONS        = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
                "SEED            = 42\n",
                "LABEL_SMOOTHING = 0.1      # FIX: Gi·∫£m t·ª´ 0.15 ‚Üí 0.1\n",
                "\n",
                "np.random.seed(SEED)\n",
                "tf.random.set_seed(SEED)\n",
                "\n",
                "CHECKPOINT_DIR  = '/content/drive/MyDrive/CaptoneProject/checkpoints/method4_fixed'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "BEST_MODEL_PATH = f'{CHECKPOINT_DIR}/best_model.keras'\n",
                "HISTORY_PATH    = f'{CHECKPOINT_DIR}/history.pkl'\n",
                "\n",
                "print(\"‚úÖ Config set!\")\n",
                "print(f\"   EPOCHS={EPOCHS}, LR={INITIAL_LR}, LABEL_SMOOTHING={LABEL_SMOOTHING}, BATCH={BATCH_SIZE}\")\n",
                "print(f\"   Loss: CategoricalCrossentropy (kh√¥ng d√πng Focal Loss)\")\n",
                "print(f\"   LR Schedule: ReduceLROnPlateau (thay Cosine Annealing)\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 6: DATA AUGMENTATION (Gi·ªØ nguy√™n t·ª´ l·∫ßn tr∆∞·ªõc)\n",
                "# ================================================\n",
                "train_datagen = ImageDataGenerator(\n",
                "    rescale           = 1./255,\n",
                "    rotation_range    = 25,\n",
                "    width_shift_range = 0.2,\n",
                "    height_shift_range= 0.2,\n",
                "    shear_range       = 0.2,\n",
                "    zoom_range        = 0.2,\n",
                "    horizontal_flip   = True,\n",
                "    brightness_range  = [0.7, 1.3],\n",
                "    fill_mode         = 'nearest',\n",
                "    validation_split  = 0.2\n",
                ")\n",
                "test_datagen = ImageDataGenerator(rescale=1./255)\n",
                "\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "    TRAIN_DIR, target_size=(IMG_SIZE, IMG_SIZE), color_mode='grayscale',\n",
                "    batch_size=BATCH_SIZE, class_mode='categorical',\n",
                "    subset='training', shuffle=True, seed=SEED\n",
                ")\n",
                "validation_generator = train_datagen.flow_from_directory(\n",
                "    TRAIN_DIR, target_size=(IMG_SIZE, IMG_SIZE), color_mode='grayscale',\n",
                "    batch_size=BATCH_SIZE, class_mode='categorical',\n",
                "    subset='validation', shuffle=False, seed=SEED\n",
                ")\n",
                "test_generator = test_datagen.flow_from_directory(\n",
                "    TEST_DIR, target_size=(IMG_SIZE, IMG_SIZE), color_mode='grayscale',\n",
                "    batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Data Generators ready!\")\n",
                "print(f\"   Train: {train_generator.samples} images\")\n",
                "print(f\"   Val:   {validation_generator.samples} images\")\n",
                "print(f\"   Test:  {test_generator.samples} images\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 7: CLASS WEIGHTS (Gi·ªØ nguy√™n)\n",
                "# ================================================\n",
                "train_labels = train_generator.classes\n",
                "class_weights_array = compute_class_weight(\n",
                "    'balanced', classes=np.unique(train_labels), y=train_labels\n",
                ")\n",
                "class_weights = dict(enumerate(class_weights_array))\n",
                "\n",
                "print(\"‚öñÔ∏è Class Weights (b√π m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu):\")\n",
                "for i, emotion in enumerate(EMOTIONS):\n",
                "    print(f\"   {emotion:10s}: {class_weights[i]:.4f}\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 8: CBAM ATTENTION + BUILD MODEL\n",
                "# (Architecture gi·ªØ nguy√™n, ch·ªâ fix Loss & LR)\n",
                "# ================================================\n",
                "\n",
                "# -------------------- CBAM ATTENTION --------------------\n",
                "class ChannelAttention(layers.Layer):\n",
                "    \"\"\"Channel Attention: d√πng c·∫£ AvgPool + MaxPool (c·∫£i ti·∫øn t·ª´ SE-Block)\"\"\"\n",
                "    def __init__(self, ratio=8, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.ratio = ratio\n",
                "\n",
                "    def build(self, input_shape):\n",
                "        ch = input_shape[-1]\n",
                "        self.dense1 = layers.Dense(ch // self.ratio, activation='relu',\n",
                "                                   kernel_initializer='he_normal')\n",
                "        self.dense2 = layers.Dense(ch, kernel_initializer='he_normal')\n",
                "        self.gap = layers.GlobalAveragePooling2D()\n",
                "        self.gmp = layers.GlobalMaxPooling2D()\n",
                "        super().build(input_shape)\n",
                "\n",
                "    def call(self, x):\n",
                "        ch = x.shape[-1]\n",
                "        avg = self.dense2(self.dense1(self.gap(x)))\n",
                "        mx  = self.dense2(self.dense1(self.gmp(x)))\n",
                "        att = tf.sigmoid(avg + mx)\n",
                "        return x * tf.reshape(att, (-1, 1, 1, ch))\n",
                "\n",
                "    def get_config(self):\n",
                "        config = super().get_config()\n",
                "        config.update({'ratio': self.ratio})\n",
                "        return config\n",
                "\n",
                "\n",
                "class SpatialAttention(layers.Layer):\n",
                "    \"\"\"Spatial Attention: focus v√†o v√πng quan tr·ªçng (m·∫Øt, mi·ªáng...)\"\"\"\n",
                "    def __init__(self, kernel_size=7, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.kernel_size = kernel_size\n",
                "\n",
                "    def build(self, input_shape):\n",
                "        self.conv = layers.Conv2D(1, self.kernel_size, padding='same',\n",
                "                                  activation='sigmoid',\n",
                "                                  kernel_initializer='he_normal')\n",
                "        super().build(input_shape)\n",
                "\n",
                "    def call(self, x):\n",
                "        avg = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
                "        mx  = tf.reduce_max (x, axis=-1, keepdims=True)\n",
                "        att = self.conv(tf.concat([avg, mx], axis=-1))\n",
                "        return x * att\n",
                "\n",
                "    def get_config(self):\n",
                "        config = super().get_config()\n",
                "        config.update({'kernel_size': self.kernel_size})\n",
                "        return config\n",
                "\n",
                "\n",
                "class CBAMBlock(layers.Layer):\n",
                "    \"\"\"CBAM = Channel Attention ‚Üí Spatial Attention\"\"\"\n",
                "    def __init__(self, ratio=8, kernel_size=7, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.ratio = ratio\n",
                "        self.kernel_size = kernel_size\n",
                "        self.ca = ChannelAttention(ratio=ratio)\n",
                "        self.sa = SpatialAttention(kernel_size=kernel_size)\n",
                "\n",
                "    def call(self, x):\n",
                "        return self.sa(self.ca(x))\n",
                "\n",
                "    def get_config(self):\n",
                "        config = super().get_config()\n",
                "        config.update({'ratio': self.ratio, 'kernel_size': self.kernel_size})\n",
                "        return config\n",
                "\n",
                "\n",
                "# -------------------- BUILD MODEL --------------------\n",
                "def build_cbam_cnn(input_shape=(48, 48, 1), num_classes=7):\n",
                "    inputs = layers.Input(shape=input_shape)\n",
                "\n",
                "    # Block 1: 64 filters ‚Äî 48x48 ‚Üí 24x24\n",
                "    x = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(inputs)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = CBAMBlock(ratio=8)(x)\n",
                "    x = layers.MaxPooling2D(2)(x)\n",
                "    x = layers.Dropout(0.25)(x)\n",
                "\n",
                "    # Block 2: 128 filters ‚Äî 24x24 ‚Üí 12x12\n",
                "    x = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = CBAMBlock(ratio=8)(x)\n",
                "    x = layers.MaxPooling2D(2)(x)\n",
                "    x = layers.Dropout(0.25)(x)\n",
                "\n",
                "    # Block 3: 256 filters ‚Äî 12x12 ‚Üí 6x6\n",
                "    x = layers.Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = CBAMBlock(ratio=16)(x)\n",
                "    x = layers.MaxPooling2D(2)(x)\n",
                "    x = layers.Dropout(0.3)(x)\n",
                "\n",
                "    # Block 4: 512 filters ‚Äî 6x6 ‚Üí 3x3\n",
                "    x = layers.Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = CBAMBlock(ratio=16)(x)\n",
                "    x = layers.MaxPooling2D(2)(x)\n",
                "    x = layers.Dropout(0.3)(x)\n",
                "\n",
                "    # Classifier Head\n",
                "    x = layers.GlobalAveragePooling2D()(x)\n",
                "    x = layers.Dense(512, kernel_regularizer=regularizers.l2(0.0005))(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Dropout(0.5)(x)\n",
                "    x = layers.Dense(256, kernel_regularizer=regularizers.l2(0.0005))(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Dropout(0.5)(x)\n",
                "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
                "\n",
                "    return models.Model(inputs=inputs, outputs=outputs)\n",
                "\n",
                "\n",
                "model = build_cbam_cnn()\n",
                "model.summary()\n",
                "print(f\"\\n‚úÖ CBAM Model built! Params: {model.count_params():,}\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 9: COMPILE\n",
                "# FIX: D√πng CategoricalCrossentropy thay FocalLoss\n",
                "# ================================================\n",
                "\n",
                "# FIX 1: S·ª≠ d·ª•ng CategoricalCrossentropy v·ªõi label_smoothing=0.1\n",
                "# (thay v√¨ FocalLoss gamma=2.0 alpha=0.25 ‚Äî qu√° aggressive)\n",
                "loss_fn = keras.losses.CategoricalCrossentropy(\n",
                "    label_smoothing=LABEL_SMOOTHING  # 0.1 ‚Äî ƒë·ªß ƒë·ªÉ regularize, kh√¥ng qu√° m·∫°nh\n",
                ")\n",
                "\n",
                "model.compile(\n",
                "    optimizer=Adam(learning_rate=INITIAL_LR),\n",
                "    loss=loss_fn,\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Model compiled!\")\n",
                "print(f\"   Loss: CategoricalCrossentropy (label_smoothing={LABEL_SMOOTHING})\")\n",
                "print(f\"   Optimizer: Adam (lr={INITIAL_LR})\")\n",
                "print(f\"   ‚ö†Ô∏è  KH√îNG d√πng FocalLoss ‚Äî nguy√™n nh√¢n g√¢y accuracy th·∫•p tr∆∞·ªõc ƒë√≥\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 10: CALLBACKS\n",
                "# FIX: ReduceLROnPlateau thay Cosine Annealing\n",
                "# ================================================\n",
                "\n",
                "class SaveHistoryCallback(Callback):\n",
                "    \"\"\"L∆∞u history sau m·ªói epoch ƒë·ªÉ kh√¥ng m·∫•t khi runtime crash\"\"\"\n",
                "    def __init__(self, path):\n",
                "        super().__init__()\n",
                "        self.path = path\n",
                "        self.data = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': [], 'lr': []}\n",
                "\n",
                "    def on_epoch_end(self, epoch, logs=None):\n",
                "        for k in ['accuracy', 'val_accuracy', 'loss', 'val_loss']:\n",
                "            self.data[k].append(logs.get(k))\n",
                "        try:\n",
                "            lr_val = float(self.model.optimizer.learning_rate)\n",
                "        except:\n",
                "            lr_val = float(self.model.optimizer.learning_rate.numpy())\n",
                "        self.data['lr'].append(lr_val)\n",
                "        with open(self.path, 'wb') as f:\n",
                "            pickle.dump(self.data, f)\n",
                "\n",
                "\n",
                "callbacks = [\n",
                "    # L∆∞u model t·ªët nh·∫•t\n",
                "    ModelCheckpoint(\n",
                "        BEST_MODEL_PATH,\n",
                "        monitor='val_accuracy', save_best_only=True, mode='max', verbose=1\n",
                "    ),\n",
                "    # D·ª´ng s·ªõm n·∫øu kh√¥ng c·∫£i thi·ªán\n",
                "    EarlyStopping(\n",
                "        monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1\n",
                "    ),\n",
                "    # FIX 2: ReduceLROnPlateau ‚Äî gi·∫£m LR khi val_loss kh√¥ng c·∫£i thi·ªán\n",
                "    # Th√≠ch nghi t·ªët h∆°n Cosine Annealing cho b√†i to√°n n√†y\n",
                "    ReduceLROnPlateau(\n",
                "        monitor='val_loss',\n",
                "        factor=0.5,       # Gi·∫£m LR c√≤n 50%\n",
                "        patience=5,       # Sau 5 epoch kh√¥ng c·∫£i thi·ªán\n",
                "        min_lr=1e-6,      # S√†n LR\n",
                "        verbose=1\n",
                "    ),\n",
                "    # L∆∞u history\n",
                "    SaveHistoryCallback(HISTORY_PATH)\n",
                "]\n",
                "\n",
                "print(\"‚úÖ Callbacks configured:\")\n",
                "print(\"   - ModelCheckpoint (monitor=val_accuracy)\")\n",
                "print(\"   - EarlyStopping (patience=15)\")\n",
                "print(\"   - ReduceLROnPlateau (factor=0.5, patience=5) ‚Üê FIX t·ª´ Cosine Annealing\")\n",
                "print(\"   - SaveHistory\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 11: TRAINING üöÄ\n",
                "# ================================================\n",
                "\n",
                "print(\"üöÄ Starting Training (Method 4 FIXED)...\")\n",
                "print(f\"   Epochs={EPOCHS}, Batch={BATCH_SIZE}\")\n",
                "print(f\"   Train={train_generator.samples}, Val={validation_generator.samples}\")\n",
                "print(f\"   Loss: CategoricalCrossentropy (label_smoothing={LABEL_SMOOTHING})\")\n",
                "print(f\"   LR: ReduceLROnPlateau (init={INITIAL_LR})\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    epochs=EPOCHS,\n",
                "    validation_data=validation_generator,\n",
                "    callbacks=callbacks,\n",
                "    class_weight=class_weights,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Training Completed!\")\n",
                "best_val_acc = max(history.history['val_accuracy'])\n",
                "best_epoch   = history.history['val_accuracy'].index(best_val_acc) + 1\n",
                "print(f\"   Best Val Accuracy: {best_val_acc*100:.2f}% (Epoch {best_epoch})\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 12: TRAINING VISUALIZATION\n",
                "# ================================================\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
                "\n",
                "# Accuracy\n",
                "axes[0].plot(history.history['accuracy'],     label='Train', linewidth=2)\n",
                "axes[0].plot(history.history['val_accuracy'], label='Val',   linewidth=2)\n",
                "best_val_acc = max(history.history['val_accuracy'])\n",
                "axes[0].axhline(best_val_acc, color='red', linestyle='--', alpha=0.5,\n",
                "                label=f'Best Val={best_val_acc*100:.2f}%')\n",
                "axes[0].set_title('Accuracy', fontsize=14)\n",
                "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Accuracy')\n",
                "axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Loss\n",
                "axes[1].plot(history.history['loss'],     label='Train', linewidth=2)\n",
                "axes[1].plot(history.history['val_loss'], label='Val',   linewidth=2)\n",
                "axes[1].set_title('Loss', fontsize=14)\n",
                "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss')\n",
                "axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "# Learning Rate\n",
                "try:\n",
                "    with open(HISTORY_PATH, 'rb') as f:\n",
                "        lr_vals = pickle.load(f).get('lr', [])\n",
                "    if lr_vals:\n",
                "        axes[2].plot(lr_vals, 'g-', linewidth=2)\n",
                "        axes[2].set_title('Learning Rate (ReduceLROnPlateau)', fontsize=14)\n",
                "        axes[2].set_xlabel('Epoch'); axes[2].set_ylabel('LR')\n",
                "        axes[2].set_yscale('log')  # Log scale cho d·ªÖ nh√¨n\n",
                "        axes[2].grid(True, alpha=0.3)\n",
                "except Exception as e:\n",
                "    print(f\"LR plot error: {e}\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{CHECKPOINT_DIR}/training_curves.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "best_val_acc = max(history.history['val_accuracy'])\n",
                "best_epoch   = history.history['val_accuracy'].index(best_val_acc) + 1\n",
                "print(f\"üìä Best Val Accuracy: {best_val_acc*100:.2f}% (Epoch {best_epoch}/{len(history.history['val_accuracy'])})\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 13: STANDARD EVALUATION\n",
                "# ================================================\n",
                "\n",
                "print(\"üìä Standard Evaluation...\")\n",
                "best_model = keras.models.load_model(\n",
                "    BEST_MODEL_PATH,\n",
                "    custom_objects={\n",
                "        'CBAMBlock':        CBAMBlock,\n",
                "        'ChannelAttention': ChannelAttention,\n",
                "        'SpatialAttention': SpatialAttention\n",
                "    }\n",
                ")\n",
                "\n",
                "test_generator.reset()\n",
                "test_loss, test_acc = best_model.evaluate(test_generator, verbose=1)\n",
                "print(f\"\\nüéØ TEST ACCURACY (Standard): {test_acc*100:.2f}%\")\n",
                "print(f\"   TEST LOSS: {test_loss:.4f}\")\n",
                "\n",
                "test_generator.reset()\n",
                "predictions = best_model.predict(test_generator, verbose=1)\n",
                "y_pred = np.argmax(predictions, axis=1)\n",
                "y_true = test_generator.classes\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"üìã Classification Report (Standard):\")\n",
                "print(\"=\" * 60)\n",
                "print(classification_report(y_true, y_pred, target_names=EMOTIONS, digits=4))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 14: TEST TIME AUGMENTATION (TTA)\n",
                "# ================================================\n",
                "\n",
                "def predict_with_tta(model, test_dir, img_size=48, batch_size=64, n_aug=5):\n",
                "    \"\"\"TTA: Trung b√¨nh d·ª± ƒëo√°n t·ª´ nhi·ªÅu augmented version\"\"\"\n",
                "    tta_datagens = [\n",
                "        ImageDataGenerator(rescale=1./255),\n",
                "        ImageDataGenerator(rescale=1./255, horizontal_flip=True),\n",
                "        ImageDataGenerator(rescale=1./255, rotation_range=10),\n",
                "        ImageDataGenerator(rescale=1./255, zoom_range=0.1),\n",
                "        ImageDataGenerator(rescale=1./255, width_shift_range=0.1, height_shift_range=0.1),\n",
                "    ]\n",
                "    all_preds = []\n",
                "    for i, dg in enumerate(tta_datagens[:n_aug]):\n",
                "        print(f\"   TTA {i+1}/{n_aug}...\", end=' ')\n",
                "        gen = dg.flow_from_directory(\n",
                "            test_dir, target_size=(img_size, img_size),\n",
                "            color_mode='grayscale', batch_size=batch_size,\n",
                "            class_mode='categorical', shuffle=False\n",
                "        )\n",
                "        preds = model.predict(gen, verbose=0)\n",
                "        all_preds.append(preds)\n",
                "        print(f\"done\")\n",
                "    return np.mean(all_preds, axis=0)\n",
                "\n",
                "\n",
                "print(\"üîÑ Running TTA (5 augmentations)...\")\n",
                "tta_preds     = predict_with_tta(best_model, TEST_DIR)\n",
                "y_pred_tta    = np.argmax(tta_preds, axis=1)\n",
                "y_true_tta    = test_generator.classes\n",
                "tta_accuracy  = np.mean(y_pred_tta == y_true_tta)\n",
                "\n",
                "print(f\"\\nüèÜ TEST ACCURACY (TTA): {tta_accuracy*100:.2f}%\")\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"üìã Classification Report (TTA):\")\n",
                "print(\"=\" * 60)\n",
                "print(classification_report(y_true_tta, y_pred_tta, target_names=EMOTIONS, digits=4))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 15: CONFUSION MATRIX\n",
                "# ================================================\n",
                "\n",
                "cm = confusion_matrix(y_true_tta, y_pred_tta)\n",
                "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
                "\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=EMOTIONS, yticklabels=EMOTIONS, ax=axes[0])\n",
                "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14)\n",
                "axes[0].set_ylabel('True'); axes[0].set_xlabel('Predicted')\n",
                "\n",
                "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Oranges',\n",
                "            xticklabels=EMOTIONS, yticklabels=EMOTIONS, ax=axes[1])\n",
                "axes[1].set_title('Confusion Matrix (Normalized %)', fontsize=14)\n",
                "axes[1].set_ylabel('True'); axes[1].set_xlabel('Predicted')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{CHECKPOINT_DIR}/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# ================================================\n",
                "# CELL 16: SO S√ÅNH K·∫æT QU·∫¢ & SAVE MODEL\n",
                "# ================================================\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"üìä SO S√ÅNH K·∫æT QU·∫¢ C√ÅC PH∆Ø∆†NG PH√ÅP\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "results = {\n",
                "    'Method 1 - Enhanced Augmentation':    62.93,\n",
                "    'Method 2 - SE Attention CNN':         64.22,\n",
                "    'Method 3 - MobileNetV2':              36.28,\n",
                "    'Method 4 (l·ªói) - Focal+Cosine':      46.30,  # K·∫øt qu·∫£ c≈©\n",
                "    'Method 4 (fixed) - Standard (TTA)':  tta_accuracy * 100,\n",
                "}\n",
                "\n",
                "print(f\"{'Method':<48s} | {'Acc':>7s} | Bar\")\n",
                "print(\"-\" * 70)\n",
                "for method, acc in results.items():\n",
                "    bar   = '‚ñà' * int(acc / 2)\n",
                "    arrow = ' ‚Üê C·∫¢I THI·ªÜN!' if 'fixed' in method else ''\n",
                "    print(f\"  {method:<46s} | {acc:6.2f}% | {bar}{arrow}\")\n",
                "\n",
                "print(\"=\" * 70)\n",
                "improvement = tta_accuracy * 100 - 64.22\n",
                "sign = '+' if improvement >= 0 else ''\n",
                "print(f\"\\n   So v·ªõi Method 2 (best tr∆∞·ªõc ƒë√¢y): {sign}{improvement:.2f}%\")\n",
                "print(f\"   So v·ªõi Method 4 l·ªói (46.3%):      +{tta_accuracy*100 - 46.30:.2f}%\")\n",
                "\n",
                "# Save\n",
                "FINAL_MODEL_PATH = '/content/drive/MyDrive/CaptoneProject/best_model_method4_fixed.keras'\n",
                "best_model.save(FINAL_MODEL_PATH)\n",
                "print(f\"\\nüíæ Model saved: {FINAL_MODEL_PATH}\")\n",
                "\n",
                "# Summary\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"üìù TRAINING SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "best_val_acc = max(history.history['val_accuracy'])\n",
                "print(f\"   Architecture:   CNN 4-Block + CBAM Attention\")\n",
                "print(f\"   Loss:           CategoricalCrossentropy + label_smoothing={LABEL_SMOOTHING}\")\n",
                "print(f\"   LR Schedule:    ReduceLROnPlateau (factor=0.5, patience=5)\")\n",
                "print(f\"   Augmentation:   Enhanced (rot=25¬∞, shift=0.2, brightness)\")\n",
                "print(f\"   Total Params:   {model.count_params():,}\")\n",
                "print(f\"   Best Val Acc:   {best_val_acc*100:.2f}%\")\n",
                "print(f\"   Test Acc (Std): {test_acc*100:.2f}%\")\n",
                "print(f\"   Test Acc (TTA): {tta_accuracy*100:.2f}%\")\n",
                "print(\"=\" * 60)\n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 4,
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        },
        "accelerator": "GPU",
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}