{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGjK1rMsciVw",
        "outputId": "b579c370-9f8a-4d86-febb-0e95729910ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "\n",
            "TensorFlow: 2.19.0\n",
            "GPUs: []\n",
            "‚ùå NO GPU! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CELL 1: KI·ªÇM TRA GPU\n",
        "# ========================================\n",
        "!nvidia-smi\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(f\"\\nTensorFlow: {tf.__version__}\")\n",
        "print(f\"GPUs: {gpus}\")\n",
        "\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    print(\"‚úÖ GPU ENABLED!\")\n",
        "else:\n",
        "    print(\"‚ùå NO GPU! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hayHMq9oXbGt",
        "outputId": "a8fd87eb-7391-4782-9e24-c085b6afcce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported!\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CELL 2: IMPORT LIBRARIES\n",
        "# ========================================\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import warnings\n",
        "import pickle\n",
        "import json\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "YFpomMPyf8T0",
        "outputId": "5f24aa45-f267-4d94-f8d9-396d8c37ffe4"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1658518819.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Drive mounted!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CELL 3: MOUNT DRIVE\n",
        "# ========================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Drive mounted!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3MCaX3riJ_u"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 3B: T·∫†O FILE ZIP - CH·ªà CH·∫†Y 1 L·∫¶N!\n",
        "# ‚ö†Ô∏è Sau khi ch·∫°y xong, KH√îNG C·∫¶N ch·∫°y l·∫°i cell n√†y n·ªØa!\n",
        "# ========================================\n",
        "import os\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/CaptoneProject/camera'\n",
        "ZIP_PATH = '/content/drive/MyDrive/CaptoneProject/camera.zip'\n",
        "\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    print(f\"‚úÖ ZIP ƒë√£ t·ªìn t·∫°i! B·ªè qua cell n√†y.\")\n",
        "    !ls -lh \"{ZIP_PATH}\"\n",
        "else:\n",
        "    print(\"üì¶ ƒêang t·∫°o ZIP (~2-3 ph√∫t)...\")\n",
        "    !cd /content/drive/MyDrive/CaptoneProject && zip -r camera.zip camera\n",
        "    print(\"‚úÖ ZIP ƒë√£ t·∫°o xong!\")\n",
        "    !ls -lh \"{ZIP_PATH}\"\n",
        "\n",
        "print(\"\\nüí° T·ª´ gi·ªù, b·ªè qua cell n√†y v√† ch·∫°y CELL 4!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDfTfINViKKG"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 4: GI·∫¢I N√âN T·ª™ ZIP + SET PATHS\n",
        "# ========================================\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "ZIP_PATH = '/content/drive/MyDrive/CaptoneProject/camera.zip'\n",
        "LOCAL_PATH = '/content/dataset'\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/CaptoneProject/checkpoints'\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(LOCAL_PATH):\n",
        "    if os.path.exists(ZIP_PATH):\n",
        "        print(\"üì¶ Gi·∫£i n√©n t·ª´ ZIP (~30 gi√¢y)...\")\n",
        "        !unzip -q \"{ZIP_PATH}\" -d /content/\n",
        "        !mv /content/camera /content/dataset\n",
        "        print(\"‚úÖ Done!\")\n",
        "    else:\n",
        "        print(\"‚ùå Kh√¥ng t√¨m th·∫•y ZIP! Ch·∫°y CELL 3B tr∆∞·ªõc.\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset ƒë√£ c√≥ s·∫µn!\")\n",
        "\n",
        "TRAIN_DIR = os.path.join(LOCAL_PATH, 'train')\n",
        "TEST_DIR = os.path.join(LOCAL_PATH, 'test')\n",
        "\n",
        "print(f\"\\nüìÅ Train: {TRAIN_DIR}\")\n",
        "print(f\"üìÅ Test: {TEST_DIR}\")\n",
        "\n",
        "# Count\n",
        "total = 0\n",
        "for folder in sorted(os.listdir(TRAIN_DIR)):\n",
        "    folder_path = os.path.join(TRAIN_DIR, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        count = len(os.listdir(folder_path))\n",
        "        total += count\n",
        "        print(f\"   {folder}: {count}\")\n",
        "print(f\"\\nüìä Total: {total} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKY7XltZXcmz"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 4: C·∫§U H√åNH\n",
        "# ========================================\n",
        "IMG_SIZE = 48\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 0.0005\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Paths cho checkpoint\n",
        "MODEL_CHECKPOINT_PATH = f'{CHECKPOINT_DIR}/emotion_model_checkpoint.keras'\n",
        "BEST_MODEL_PATH = f'{CHECKPOINT_DIR}/emotion_best_model.keras'\n",
        "HISTORY_PATH = f'{CHECKPOINT_DIR}/training_history.pkl'\n",
        "CONFIG_PATH = f'{CHECKPOINT_DIR}/config.json'\n",
        "\n",
        "# L∆∞u config\n",
        "config = {\n",
        "    'IMG_SIZE': IMG_SIZE,\n",
        "    'BATCH_SIZE': BATCH_SIZE,\n",
        "    'EPOCHS': EPOCHS,\n",
        "    'LEARNING_RATE': LEARNING_RATE,\n",
        "    'NUM_CLASSES': NUM_CLASSES,\n",
        "    'EMOTIONS': EMOTIONS\n",
        "}\n",
        "with open(CONFIG_PATH, 'w') as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "print(\"‚úÖ Config set and saved!\")\n",
        "print(f\"   Image: {IMG_SIZE}x{IMG_SIZE} | Batch: {BATCH_SIZE} | LR: {LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBk7WKMEbvGn"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 5: DATA GENERATORS\n",
        "# ========================================\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"üìÇ Loading data from LOCAL storage...\")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Train: {train_generator.samples} | Val: {validation_generator.samples} | Test: {test_generator.samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cfx-HhfZX0He"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 6: CLASS WEIGHTS\n",
        "# ========================================\n",
        "train_labels = train_generator.classes\n",
        "class_weights_array = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "class_weights = dict(enumerate(class_weights_array))\n",
        "\n",
        "print(\"üìä Class Weights:\")\n",
        "for idx, emotion in enumerate(EMOTIONS):\n",
        "    print(f\"   {emotion}: {class_weights[idx]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxGRnxgUX65X"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 7: BUILD MODEL (ho·∫∑c LOAD T·ª™ CHECKPOINT)\n",
        "# ========================================\n",
        "\n",
        "def build_cnn(input_shape=(48, 48, 1), num_classes=7):\n",
        "    model = models.Sequential([\n",
        "        # Block 1\n",
        "        layers.Conv2D(64, (3, 3), padding='same', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.Conv2D(64, (3, 3), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Block 2\n",
        "        layers.Conv2D(128, (3, 3), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.Conv2D(128, (3, 3), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Block 3\n",
        "        layers.Conv2D(256, (3, 3), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.Conv2D(256, (3, 3), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Block 4\n",
        "        layers.Conv2D(512, (3, 3), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.Conv2D(512, (3, 3), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Classifier\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(256, kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Ki·ªÉm tra c√≥ checkpoint kh√¥ng\n",
        "initial_epoch = 0\n",
        "history_data = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': [], 'lr': []}\n",
        "\n",
        "if os.path.exists(MODEL_CHECKPOINT_PATH):\n",
        "    print(\"üìÇ Found checkpoint! Loading...\")\n",
        "    model = keras.models.load_model(MODEL_CHECKPOINT_PATH)\n",
        "\n",
        "    # Load history\n",
        "    if os.path.exists(HISTORY_PATH):\n",
        "        with open(HISTORY_PATH, 'rb') as f:\n",
        "            history_data = pickle.load(f)\n",
        "        initial_epoch = len(history_data['accuracy'])\n",
        "        print(f\"‚úÖ Resuming from epoch {initial_epoch}\")\n",
        "        print(f\"   Last val_accuracy: {history_data['val_accuracy'][-1]*100:.2f}%\")\n",
        "else:\n",
        "    print(\"üÜï No checkpoint found. Building new model...\")\n",
        "    model = build_cnn()\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "print(f\"\\n‚úÖ Model ready: {model.count_params():,} parameters\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT3PzsgCX9w_"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 8: CALLBACKS V·ªöI AUTO-SAVE\n",
        "# ========================================\n",
        "\n",
        "# Custom callback ƒë·ªÉ l∆∞u history sau m·ªói epoch\n",
        "class SaveHistoryCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, history_path, checkpoint_path):\n",
        "        super().__init__()\n",
        "        self.history_path = history_path\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "        self.history_data = history_data  # S·ª≠ d·ª•ng history ƒë√£ load\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # C·∫≠p nh·∫≠t history\n",
        "        self.history_data['accuracy'].append(logs.get('accuracy'))\n",
        "        self.history_data['val_accuracy'].append(logs.get('val_accuracy'))\n",
        "        self.history_data['loss'].append(logs.get('loss'))\n",
        "        self.history_data['val_loss'].append(logs.get('val_loss'))\n",
        "        self.history_data['lr'].append(float(self.model.optimizer.learning_rate.numpy()))\n",
        "\n",
        "        # L∆∞u history\n",
        "        with open(self.history_path, 'wb') as f:\n",
        "            pickle.dump(self.history_data, f)\n",
        "\n",
        "        # L∆∞u model checkpoint\n",
        "        self.model.save(self.checkpoint_path)\n",
        "        print(f\"   üíæ Checkpoint saved to Drive!\")\n",
        "\n",
        "callbacks = [\n",
        "    # L∆∞u best model\n",
        "    ModelCheckpoint(\n",
        "        filepath=BEST_MODEL_PATH,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Early stopping\n",
        "    EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Reduce LR\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Auto-save sau m·ªói epoch\n",
        "    SaveHistoryCallback(HISTORY_PATH, MODEL_CHECKPOINT_PATH)\n",
        "]\n",
        "\n",
        "print(\"‚úÖ Callbacks configured with AUTO-SAVE!\")\n",
        "print(f\"   üìÅ Checkpoint: {MODEL_CHECKPOINT_PATH}\")\n",
        "print(f\"   üìÅ Best model: {BEST_MODEL_PATH}\")\n",
        "print(f\"   üìÅ History: {HISTORY_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfeId5W6YIDw"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 9: TRAINING (C√ì TH·ªÇ RESUME)\n",
        "# ========================================\n",
        "print(\"üöÄ Starting Training...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if initial_epoch > 0:\n",
        "    print(f\"‚è© RESUMING from epoch {initial_epoch}\")\n",
        "else:\n",
        "    print(\"üÜï Starting from scratch\")\n",
        "\n",
        "print(f\"   Epochs: {initial_epoch} ‚Üí {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "train_generator.reset()\n",
        "validation_generator.reset()\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    initial_epoch=initial_epoch,  # Resume t·ª´ epoch ƒë√£ train\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ TRAINING COMPLETED!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWT7QmW7YLhR"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 10: V·∫º TRAINING CURVES\n",
        "# ========================================\n",
        "\n",
        "# Load full history t·ª´ file\n",
        "with open(HISTORY_PATH, 'rb') as f:\n",
        "    full_history = pickle.load(f)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(full_history['accuracy'], 'b-', linewidth=2, label='Train')\n",
        "axes[0].plot(full_history['val_accuracy'], 'r-', linewidth=2, label='Validation')\n",
        "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(full_history['loss'], 'b-', linewidth=2, label='Train')\n",
        "axes[1].plot(full_history['val_loss'], 'r-', linewidth=2, label='Validation')\n",
        "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CHECKPOINT_DIR}/training_curves.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Print best results\n",
        "best_epoch = np.argmax(full_history['val_accuracy'])\n",
        "print(f\"\\nüèÜ Best Epoch: {best_epoch + 1}\")\n",
        "print(f\"   Best Val Accuracy: {full_history['val_accuracy'][best_epoch]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvk4x9lJYQbq"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 11: EVALUATE TR√äN TEST SET\n",
        "# ========================================\n",
        "\n",
        "# Load best model\n",
        "print(\"üìÇ Loading best model...\")\n",
        "best_model = keras.models.load_model(BEST_MODEL_PATH)\n",
        "\n",
        "print(\"\\nüìä Evaluating on Test Set...\")\n",
        "test_generator.reset()\n",
        "test_loss, test_acc = best_model.evaluate(test_generator)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"üéØ TEST ACCURACY: {test_acc*100:.2f}%\")\n",
        "print(f\"üìâ TEST LOSS: {test_loss:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFPDdlaiYUOi"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 12: CONFUSION MATRIX + CLASSIFICATION REPORT\n",
        "# ========================================\n",
        "\n",
        "# Predictions\n",
        "test_generator.reset()\n",
        "predictions = best_model.predict(test_generator, verbose=1)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nüìã CLASSIFICATION REPORT:\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_true, y_pred, target_names=EMOTIONS, digits=4))\n",
        "\n",
        "# Per-class accuracy\n",
        "print(\"\\nüìä PER-CLASS ACCURACY:\")\n",
        "for i, emotion in enumerate(EMOTIONS):\n",
        "    class_total = np.sum(y_true == i)\n",
        "    class_correct = np.sum((y_true == i) & (y_pred == i))\n",
        "    class_acc = class_correct / class_total * 100 if class_total > 0 else 0\n",
        "    print(f\"   {emotion:10s}: {class_acc:5.1f}% ({class_correct}/{class_total})\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=EMOTIONS, yticklabels=EMOTIONS,\n",
        "            annot_kws={'size': 12})\n",
        "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CHECKPOINT_DIR}/confusion_matrix.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4UrEkWPbsB1"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 13: L∆ØU MODEL FINAL V√ÄO DRIVE\n",
        "# ========================================\n",
        "\n",
        "FINAL_SAVE_DIR = '/content/drive/MyDrive/CaptoneProject/trained_models'\n",
        "os.makedirs(FINAL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Copy all files\n",
        "print(\"üíæ Saving final model and files to Drive...\")\n",
        "\n",
        "# 1. Best model\n",
        "shutil.copy(BEST_MODEL_PATH, f'{FINAL_SAVE_DIR}/emotion_model_final.keras')\n",
        "print(f\"   ‚úÖ Model: {FINAL_SAVE_DIR}/emotion_model_final.keras\")\n",
        "\n",
        "# 2. Training curves\n",
        "shutil.copy(f'{CHECKPOINT_DIR}/training_curves.png', f'{FINAL_SAVE_DIR}/')\n",
        "print(f\"   ‚úÖ Training curves saved\")\n",
        "\n",
        "# 3. Confusion matrix\n",
        "shutil.copy(f'{CHECKPOINT_DIR}/confusion_matrix.png', f'{FINAL_SAVE_DIR}/')\n",
        "print(f\"   ‚úÖ Confusion matrix saved\")\n",
        "\n",
        "# 4. Training history as CSV\n",
        "history_df = pd.DataFrame(full_history)\n",
        "history_df.to_csv(f'{FINAL_SAVE_DIR}/training_history.csv', index=False)\n",
        "print(f\"   ‚úÖ History CSV saved\")\n",
        "\n",
        "# 5. Config\n",
        "shutil.copy(CONFIG_PATH, f'{FINAL_SAVE_DIR}/')\n",
        "print(f\"   ‚úÖ Config saved\")\n",
        "\n",
        "# 6. Summary\n",
        "summary = {\n",
        "    'best_epoch': int(best_epoch + 1),\n",
        "    'best_val_accuracy': float(full_history['val_accuracy'][best_epoch]),\n",
        "    'test_accuracy': float(test_acc),\n",
        "    'test_loss': float(test_loss),\n",
        "    'total_epochs': len(full_history['accuracy']),\n",
        "    'model_params': model.count_params()\n",
        "}\n",
        "with open(f'{FINAL_SAVE_DIR}/summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(f\"   ‚úÖ Summary saved\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ ALL FILES SAVED!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"üìÅ Location: {FINAL_SAVE_DIR}\")\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(f\"   - emotion_model_final.keras\")\n",
        "print(f\"   - training_curves.png\")\n",
        "print(f\"   - confusion_matrix.png\")\n",
        "print(f\"   - training_history.csv\")\n",
        "print(f\"   - config.json\")\n",
        "print(f\"   - summary.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caQ6Wk2RYWc6"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL BONUS: LOAD MODEL ƒê√É TRAIN\n",
        "# Ch·∫°y cell n√†y n·∫øu b·∫°n mu·ªën load model ƒë√£ train xong\n",
        "# ========================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import json\n",
        "\n",
        "FINAL_SAVE_DIR = '/content/drive/MyDrive/CaptoneProject/trained_models'\n",
        "\n",
        "# Load model\n",
        "print(\"üìÇ Loading trained model...\")\n",
        "model = keras.models.load_model(f'{FINAL_SAVE_DIR}/emotion_model_final.keras')\n",
        "print(\"‚úÖ Model loaded!\")\n",
        "\n",
        "# Load summary\n",
        "with open(f'{FINAL_SAVE_DIR}/summary.json', 'r') as f:\n",
        "    summary = json.load(f)\n",
        "\n",
        "print(f\"\\nüìä Model Summary:\")\n",
        "print(f\"   Best Val Accuracy: {summary['best_val_accuracy']*100:.2f}%\")\n",
        "print(f\"   Test Accuracy: {summary['test_accuracy']*100:.2f}%\")\n",
        "print(f\"   Parameters: {summary['model_params']:,}\")\n",
        "\n",
        "# Emotions\n",
        "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "print(f\"   Classes: {EMOTIONS}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
