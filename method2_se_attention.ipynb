{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Method 2: SE-Attention - FER2013 Training"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPUs: {gpus}\")\n",
    "\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"\u2705 GPU ENABLED!\")\n",
    "else:\n",
    "    print(\"\u274c NO GPU! Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "print(\"\u2705 Libraries imported!\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"\u2705 Drive mounted!\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "DATASET_PATH = '/content/drive/MyDrive/CaptoneProject/camera'\n",
    "ZIP_PATH = '/content/drive/MyDrive/CaptoneProject/camera.zip'\n",
    "\n",
    "if os.path.exists(ZIP_PATH):\n",
    "    print(f\"\u2705 ZIP found at {ZIP_PATH}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f ZIP not found! Please run the specific zip creation cell if needed.\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "ZIP_PATH = '/content/drive/MyDrive/CaptoneProject/camera.zip'\n",
    "LOCAL_PATH = '/content/dataset'\n",
    "\n",
    "if not os.path.exists(LOCAL_PATH):\n",
    "    if os.path.exists(ZIP_PATH):\n",
    "        print(\"\ud83d\udce6 Unzipping... (this might take a moment)\")\n",
    "        !unzip -q -o \"{ZIP_PATH}\" -d /content/\n",
    "        \n",
    "        # Handle directory structure\n",
    "        if os.path.exists('/content/camera'):\n",
    "            !mv /content/camera \"{LOCAL_PATH}\"\n",
    "        elif os.path.exists('/content/train') and os.path.exists('/content/test'):\n",
    "            os.makedirs(LOCAL_PATH, exist_ok=True)\n",
    "            !mv /content/train \"{LOCAL_PATH}/train\"\n",
    "            !mv /content/test \"{LOCAL_PATH}/test\"\n",
    "        print(\"\u2705 Dataset ready at /content/dataset\")\n",
    "    else:\n",
    "        print(\"\u274c ZIP file not found in Drive!\")\n",
    "else:\n",
    "    print(\"\u2705 Dataset already exists locally!\")\n",
    "\n",
    "TRAIN_DIR = os.path.join(LOCAL_PATH, 'train')\n",
    "TEST_DIR = os.path.join(LOCAL_PATH, 'test')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CONFIG: METHOD 2 - SE ATTENTION CNN\n",
    "# ========================================\n",
    "IMG_SIZE = 48\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0005\n",
    "NUM_CLASSES = 7\n",
    "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "SEED = 42\n",
    "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/CaptoneProject/checkpoints/method2_se'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "MODEL_CHECKPOINT_PATH = f'{CHECKPOINT_DIR}/checkpoint.keras'\n",
    "BEST_MODEL_PATH = f'{CHECKPOINT_DIR}/best_model.keras'\n",
    "HISTORY_PATH = f'{CHECKPOINT_DIR}/history.pkl'\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Standard Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, rotation_range=15, width_shift_range=0.15, height_shift_range=0.15,\n",
    "    shear_range=0.15, zoom_range=0.15, horizontal_flip=True, fill_mode='nearest', validation_split=0.2\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAIN_DIR, target_size=(48,48), color_mode='grayscale', batch_size=BATCH_SIZE, class_mode='categorical', subset='training', seed=SEED)\n",
    "validation_generator = train_datagen.flow_from_directory(TRAIN_DIR, target_size=(48,48), color_mode='grayscale', batch_size=BATCH_SIZE, class_mode='categorical', subset='validation', seed=SEED)\n",
    "test_generator = test_datagen.flow_from_directory(TEST_DIR, target_size=(48,48), color_mode='grayscale', batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train_labels = train_generator.classes\n",
    "class_weights_array = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights = dict(enumerate(class_weights_array))\n",
    "\n",
    "print(\"\ud83d\udcca Class Weights calculated!\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def squeeze_excitation_block(input_tensor, ratio=16):\n",
    "    channels = input_tensor.shape[-1]\n",
    "    se = layers.GlobalAveragePooling2D()(input_tensor)\n",
    "    se = layers.Dense(channels // ratio, activation='relu')(se)\n",
    "    se = layers.Dense(channels, activation='sigmoid')(se)\n",
    "    se = layers.Reshape((1, 1, channels))(se)\n",
    "    return layers.Multiply()([input_tensor, se])\n",
    "\n",
    "def build_se_cnn(input_shape=(48, 48, 1), num_classes=7):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Block 1\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = squeeze_excitation_block(x, ratio=8)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = squeeze_excitation_block(x, ratio=8)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = squeeze_excitation_block(x, ratio=16)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = squeeze_excitation_block(x, ratio=16)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Classifier using GAP\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(256, kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = build_se_cnn()\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])\n",
    "model.summary()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class SaveHistoryCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, history_path, checkpoint_path):\n",
    "        super().__init__()\n",
    "        self.history_path = history_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.history_data = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': [], 'lr': []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.history_data['accuracy'].append(logs.get('accuracy'))\n",
    "        self.history_data['val_accuracy'].append(logs.get('val_accuracy'))\n",
    "        self.history_data['loss'].append(logs.get('loss'))\n",
    "        self.history_data['val_loss'].append(logs.get('val_loss'))\n",
    "        self.history_data['lr'].append(float(self.model.optimizer.learning_rate.numpy()))\n",
    "\n",
    "        with open(self.history_path, 'wb') as f:\n",
    "            pickle.dump(self.history_data, f)\n",
    "        # Save checkpoint periodically or on best is handled by ModelCheckpoint, \n",
    "        # but we can save here too if needed. \n",
    "        # self.model.save(self.checkpoint_path)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=BEST_MODEL_PATH, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-7, verbose=1),\n",
    "    SaveHistoryCallback(HISTORY_PATH, MODEL_CHECKPOINT_PATH)\n",
    "]\n",
    "print(\"\u2705 Callbacks configured!\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\ud83d\ude80 Starting Training...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\u2705 Training Completed!\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Load best model\n",
    "best_model = keras.models.load_model(BEST_MODEL_PATH)\n",
    "\n",
    "test_generator.reset()\n",
    "test_loss, test_acc = best_model.evaluate(test_generator)\n",
    "print(f\"\\n\ud83c\udfaf TEST ACCURACY: {test_acc*100:.2f}%\")\n",
    "print(f\"   TEST LOSS: {test_loss:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "predictions = best_model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=EMOTIONS, digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=EMOTIONS, yticklabels=EMOTIONS)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}